# RAG System Configuration

# Data paths
data_directory: "data"
vector_db_directory: "vector_db"

# Embedding model settings - set to Azure AI Inference by default
embedding_model:
  type: "azure_inference"
  endpoint: "https://models.inference.ai.azure.com"
  model_name: "text-embedding-3-small"
  kwargs:
    device: "cpu"

# Default text splitter settings (used as fallback)
text_splitter:
  chunk_size: 1000
  chunk_overlap: 200

# File-specific processing strategies
file_processors:
  pdf:
    processor: "auto_detect"
    text_splitter:
      chunk_size: 1000
      chunk_overlap: 200
  docx:
    processor: "auto_detect"
    text_splitter:
      chunk_size: 800
      chunk_overlap: 150
  txt:
    processor: "auto_detect"
    text_splitter:
      chunk_size: 1200
      chunk_overlap: 250
  csv:
    processor: "structured"
    separator: ","
  md:
    processor: "auto_detect"
    text_splitter:
      chunk_size: 500
      chunk_overlap: 100

# Parallel processing settings
parallel_processing:
  enabled: true
  max_workers: 4

# Retrieval settings
retrieval:
  top_k: 5

# LLM settings - using Azure AI Inference exclusively
llm:
  type: "azure_inference"
  endpoint: "https://models.inference.ai.azure.com"
  model_name: "gpt-4o"
  temperature: 0.7
  max_tokens: 1000
  top_p: 1.0
  system_message: "You are a helpful assistant that provides accurate, factual information based on the provided context."

# Chat settings
max_chat_history: 10
